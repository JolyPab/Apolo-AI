import os
import json
from dotenv import load_dotenv
from docx import Document
from langchain_openai import AzureOpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from tqdm import tqdm
import base64

"""–°–∫—Ä–∏–ø—Ç —Å–æ–∑–¥–∞—ë—Ç FAISS-–∏–Ω–¥–µ–∫—Å –∏–∑ —Ç–µ–∫—Å—Ç–∞ .docx-—Ñ–∞–π–ª–∞
Usage:
    python create_doc_embeddings.py <path_to_docx> [index_dir]
–ü–æ —É–º–æ–ª—á–∞–Ω–∏—é index_dir = "doc_faiss".
"""

import sys

# --- cli args ---------------------------------------------------------------
if len(sys.argv) < 2:
    print("Uso: python create_doc_embeddings.py <archivo.docx> [directorio_indice]")
    sys.exit(1)

docx_path = sys.argv[1]
index_dir = sys.argv[2] if len(sys.argv) > 2 else "doc_faiss"
images_dir = f"{index_dir}_images"

# --- env & embeddings -------------------------------------------------------
load_dotenv()
embeddings_model = AzureOpenAIEmbeddings(
    api_key=os.getenv("AZURE_EMBEDDINGS_API_KEY"),
    azure_endpoint=os.getenv("AZURE_EMBEDDINGS_ENDPOINT"),
    deployment="text-embedding-ada-002",
    api_version="2023-05-15",
)

# --- extract text -----------------------------------------------------------
print(f"üìñ Extrayendo texto de {docx_path}‚Ä¶")
doc = Document(docx_path)
paragraphs = [p.text.strip() for p in doc.paragraphs if p.text.strip()]

# --- extract text from tables ---------------------------------------------
for tbl in doc.tables:
    for row in tbl.rows:
        cells = [c.text.strip() for c in row.cells if c.text.strip()]
        if cells:
            paragraphs.append(" | ".join(cells))

# --- extract images -------------------------------------------------------
print(f"üñºÔ∏è Extrayendo im√°genes...")
os.makedirs(images_dir, exist_ok=True)
image_info = []

# –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ relationships
for rel in doc.part.rels.values():
    if "image" in rel.target_ref:
        try:
            image_data = rel.target_part.blob
            image_ext = rel.target_ref.split('.')[-1]
            image_filename = f"image_{len(image_info)+1}.{image_ext}"
            image_path = os.path.join(images_dir, image_filename)
            
            with open(image_path, 'wb') as f:
                f.write(image_data)
            
            image_info.append({
                "filename": image_filename,
                "path": image_path,
                "size": len(image_data)
            })
            print(f"  ‚úÖ {image_filename} ({len(image_data)} bytes)")
        except Exception as e:
            print(f"  ‚ùå Error extrayendo imagen: {e}")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö
with open(f"{images_dir}/images_info.json", "w", encoding="utf-8") as f:
    json.dump(image_info, f, ensure_ascii=False, indent=2)

print(f"üí° {len(paragraphs)} p√°rrafos encontrados. Generando embeddings‚Ä¶")

# --- chunking ---------------------------------------------------------------
CHUNK_SIZE = 4  # –∞–±–∑–∞—Ü–µ–≤ –≤ –æ–¥–Ω–æ–º —á–∞–Ω–∫–µ
faiss_index = None
chunk = []

for para in tqdm(paragraphs, desc="Embeddings"):
    chunk.append(para)
    if len(chunk) >= CHUNK_SIZE:
        text = "\n".join(chunk)
        emb = FAISS.from_texts([text], embeddings_model)
        if faiss_index is None:
            faiss_index = emb
        else:
            faiss_index.merge_from(emb)
        chunk = []

# –æ—Å—Ç–∞—Ç–æ–∫
if chunk:
    text = "\n".join(chunk)
    emb = FAISS.from_texts([text], embeddings_model)
    if faiss_index is None:
        faiss_index = emb
    else:
        faiss_index.merge_from(emb)

# --- save -------------------------------------------------------------------
faiss_index.save_local(index_dir)
print(f"‚úÖ √çndice guardado en {index_dir}")
print(f"‚úÖ {len(image_info)} im√°genes guardadas en {images_dir}") 