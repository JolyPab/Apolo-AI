import streamlit as st
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain_community.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from datetime import datetime
import os
import json
import base64
from dotenv import load_dotenv

# --- env --------------------------------------------------------------------
load_dotenv()

# --- embeddings & index -----------------------------------------------------
embeddings = AzureOpenAIEmbeddings(
    api_key=os.getenv("AZURE_EMBEDDINGS_API_KEY"),
    azure_endpoint=os.getenv("AZURE_EMBEDDINGS_ENDPOINT"),
    deployment="text-embedding-ada-002",
    api_version="2023-05-15",
)

INDEX_DIR = "doc_faiss"  # –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ –∏–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω –∑–∞—Ä–∞–Ω–µ–µ
IMAGES_DIR = f"{INDEX_DIR}_images"
index = FAISS.load_local(INDEX_DIR, embeddings, allow_dangerous_deserialization=True)

# --- load images info -------------------------------------------------------
images_info = []
images_info_path = os.path.join(IMAGES_DIR, "images_info.json")

if os.path.exists(images_info_path):
    with open(images_info_path, "r", encoding="utf-8") as f:
        images_info = json.load(f)

def encode_image_to_base64(image_path):
    """–ö–æ–¥–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ base64 –¥–ª—è Vision API"""
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

def analyze_image_with_vision(image_path, question):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é GPT-4V"""
    try:
        # –°–æ–∑–¥–∞—ë–º –∫–ª–∏–µ–Ω—Ç –¥–ª—è Vision API
        vision_llm = AzureChatOpenAI(
            api_key=os.getenv("AZURE_OPENAI_API_KEY"),
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
            azure_deployment=os.getenv("AZURE_VISION_DEPLOYMENT", "gpt-4-vision"),  # –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è
            api_version="2024-02-15-preview",
            max_tokens=500
        )
        
        # –ö–æ–¥–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        base64_image = encode_image_to_base64(image_path)
        
        # –°–æ–∑–¥–∞—ë–º —Å–æ–æ–±—â–µ–Ω–∏–µ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º
        messages = [
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": f"Analiza esta imagen del documento t√©cnico y responde: {question}. Describe qu√© se muestra en la imagen de manera detallada y t√©cnica."
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/png;base64,{base64_image}"
                        }
                    }
                ]
            }
        ]
        
        response = vision_llm.invoke(messages)
        return response.content
        
    except Exception as e:
        return f"No se pudo analizar la imagen: {str(e)}"

def get_related_images(question, answer):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –∫–∞–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å–≤—è–∑–∞–Ω—ã —Å –≤–æ–ø—Ä–æ—Å–æ–º/–æ—Ç–≤–µ—Ç–æ–º"""
    if not images_info:
        return []
    
    text_to_check = (question + " " + answer).lower()
    related_images = []
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å–Ω–∞—á–∞–ª–∞
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ figura 2 (–¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ü–ï–†–í–û–ô, —á—Ç–æ–±—ã –ø–µ—Ä–µ—Ö–≤–∞—Ç–∏—Ç—å —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ)
    if "figura 2" in text_to_check:
        if len(images_info) > 1:
            related_images.append(images_info[1]["filename"])
        return related_images
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ figura 1
    if "figura 1" in text_to_check:
        related_images.extend([images_info[0]["filename"], images_info[-1]["filename"]])
        return related_images
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ–¥—É—Ä—ã GTAW –±–µ–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ñ–∏–≥—É—Ä—ã
    if any(word in text_to_check for word in ["gtaw", "proceso", "t√©cnica"]) and "figura" not in text_to_check:
        if len(images_info) > 1:
            related_images.append(images_info[1]["filename"])
        return related_images
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –±–∏—Å–µ–ª—è –±–µ–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ñ–∏–≥—É—Ä—ã
    if any(word in text_to_check for word in ["bisel", "geometr√≠a"]) and "figura" not in text_to_check:
        related_images.extend([images_info[0]["filename"], images_info[-1]["filename"]])
        return related_images
    
    # –ï—Å–ª–∏ —Å–ø—Ä–∞—à–∏–≤–∞—é—Ç –ø–æ–∫–∞–∑–∞—Ç—å –≤—Å–µ –¥–∏–∞–≥—Ä–∞–º–º—ã
    if any(word in text_to_check for word in ["todas las figuras", "mostrar todas", "ver todas"]):
        return [img["filename"] for img in images_info]
    
    # Fallback: –µ—Å–ª–∏ AI –Ω–µ –º–æ–∂–µ—Ç –ø–æ–∫–∞–∑–∞—Ç—å, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    if any(phrase in answer.lower() for phrase in ["no tengo acceso", "no dispongo", "no puedo mostrar"]):
        if any(word in text_to_check for word in ["figura", "imagen", "diagrama"]):
            related_images.append(images_info[0]["filename"])
    
    return related_images

# --- llm --------------------------------------------------------------------
llm = AzureChatOpenAI(
    api_key=os.getenv("AZURE_OPENAI_API_KEY"),
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
    azure_deployment="gpt-4",
    api_version="2024-02-15-preview",
)

# --- prompt -----------------------------------------------------------------
current_date = datetime.now().strftime("%Y-%m-%d")
TEMPLATE = """
Eres un asistente t√©cnico. Responde √∫nicamente bas√°ndote en el contenido del documento proporcionado.
Si la pregunta no est√° relacionada o la informaci√≥n no se encuentra en el documento, responde educadamente que no dispones de datos.

El documento tambi√©n contiene {num_images} im√°genes/diagramas que pueden ser relevantes para las consultas.

Fecha actual: {current_date}

Historial del di√°logo:
{chat_history}

Contexto del documento:
{context}

Pregunta del usuario: {question}
Respuesta:
"""
PROMPT = PromptTemplate(
    input_variables=["context", "chat_history", "current_date", "question", "num_images"],
    template=TEMPLATE,
).partial(current_date=current_date, num_images=len(images_info))

# --- memory & chain ---------------------------------------------------------
if "memory" not in st.session_state:
    st.session_state["memory"] = ConversationBufferMemory(
        memory_key="chat_history",
        input_key="question",
        output_key="answer",
        return_messages=True,
    )

# --- chat history -----------------------------------------------------------
if "messages" not in st.session_state:
    st.session_state.messages = []

qa = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=index.as_retriever(search_kwargs={"k": 20}),
    memory=st.session_state["memory"],
    combine_docs_chain_kwargs={"prompt": PROMPT},
)

# --- UI ---------------------------------------------------------------------
st.set_page_config(page_title="Asistente Documento", page_icon="üìÑ")
st.title("üìÑ Asistente del Documento")

# –ü–æ–∫–∞–∑–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ –≤ —Å–∞–π–¥–±–∞—Ä–µ
with st.sidebar:
    st.markdown("### üìã Informaci√≥n del documento")
    if images_info:
        st.markdown(f"üñºÔ∏è **Im√°genes encontradas:** {len(images_info)}")
        
        # –ü–æ–∫–∞–∑–∞—Ç—å –ø—Ä–µ–≤—å—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        st.markdown("### üñºÔ∏è Im√°genes del documento")
        for i, img_info in enumerate(images_info):
            img_path = os.path.join(IMAGES_DIR, img_info["filename"])
            if os.path.exists(img_path):
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –ª–∏ —Ñ–æ—Ä–º–∞—Ç Streamlit
                file_ext = img_info["filename"].lower().split('.')[-1]
                if file_ext in ['png', 'jpg', 'jpeg', 'gif', 'bmp', 'svg']:
                    with st.expander(f"Imagen {i+1} ({img_info['size']} bytes)"):
                        try:
                            st.image(img_path, use_container_width=True)
                        except Exception as e:
                            st.error(f"No se puede mostrar la imagen: {img_info['filename']}")
                else:
                    with st.expander(f"Imagen {i+1} ({img_info['size']} bytes) - {file_ext.upper()}"):
                        st.warning(f"Formato {file_ext.upper()} no soportado para vista previa. Archivo: {img_info['filename']}")
    else:
        st.markdown("üñºÔ∏è **Im√°genes:** No encontradas")
    
    # –ö–Ω–æ–ø–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∏—Å—Ç–æ—Ä–∏–∏
    if st.button("üóëÔ∏è Limpiar historial"):
        st.session_state.messages = []
        st.session_state["memory"].clear()
        st.rerun()

# –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# –ü–æ–ª–µ –≤–≤–æ–¥–∞
if prompt := st.chat_input("Preg√∫ntame sobre el documento‚Ä¶"):
    # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –∏—Å—Ç–æ—Ä–∏—é
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç AI
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∑–∫–∏
        message_placeholder.markdown("ü§î Pensando...")
        
        # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç
        try:
            result = qa.invoke({"question": prompt})
            respuesta = result["answer"]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–µ–Ω –ª–∏ –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ñ–∏–≥—É—Ä—ã —Å –ø–æ–º–æ—â—å—é Vision API
            vision_analysis = ""
            if any(phrase in prompt.lower() for phrase in ["figura 1", "figura 2", "figura 3", "qu√© muestra", "describe la imagen"]):
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞–∫–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å
                image_to_analyze = None
                if "figura 1" in prompt.lower() and len(images_info) > 0:
                    image_to_analyze = os.path.join(IMAGES_DIR, images_info[0]["filename"])
                elif "figura 2" in prompt.lower() and len(images_info) > 1:
                    image_to_analyze = os.path.join(IMAGES_DIR, images_info[1]["filename"])
                elif "figura 3" in prompt.lower() and len(images_info) > 2:
                    image_to_analyze = os.path.join(IMAGES_DIR, images_info[2]["filename"])
                
                if image_to_analyze and os.path.exists(image_to_analyze):
                    with st.status("üîç Analizando imagen con GPT-4V...", expanded=False):
                        vision_analysis = analyze_image_with_vision(image_to_analyze, prompt)
                    
                    # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –∏–∑ —Ç–µ–∫—Å—Ç–∞ –∏ –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    if vision_analysis and "No se pudo analizar" not in vision_analysis:
                        respuesta = f"{respuesta}\n\n**üì∏ An√°lisis de la imagen:**\n{vision_analysis}"
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –ø–æ–∫–∞–∑–∞—Ç—å —Å–≤—è–∑–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            related_images = get_related_images(prompt, respuesta)
            
            # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –æ—Ç–≤–µ—Ç
            message_placeholder.markdown(respuesta)
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–≤—è–∑–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –µ—Å–ª–∏ –µ—Å—Ç—å
            if related_images:
                st.markdown("### üñºÔ∏è Im√°genes relacionadas:")
                cols = st.columns(min(len(related_images), 3))  # –º–∞–∫—Å–∏–º—É–º 3 –∫–æ–ª–æ–Ω–∫–∏
                for i, img_filename in enumerate(related_images):
                    img_path = os.path.join(IMAGES_DIR, img_filename)
                    if os.path.exists(img_path):
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞
                        file_ext = img_filename.lower().split('.')[-1]
                        if file_ext in ['png', 'jpg', 'jpeg', 'gif', 'bmp', 'svg']:
                            with cols[i % 3]:
                                try:
                                    st.image(img_path, caption=f"Imagen {i+1}", use_container_width=True)
                                except Exception as e:
                                    st.error(f"Error mostrando {img_filename}")
                        else:
                            with cols[i % 3]:
                                st.warning(f"Formato {file_ext.upper()} no soportado")
                                st.caption(f"Archivo: {img_filename}")
                            
        except Exception as e:
            respuesta = f"‚ùå Error: {str(e)}\n\nRevisa las claves de Azure OpenAI en los secretos."
            message_placeholder.markdown(respuesta)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç AI –≤ –∏—Å—Ç–æ—Ä–∏—é
    st.session_state.messages.append({"role": "assistant", "content": respuesta}) 